@misc{bowman2015,
    author = {Samuel R. Bowman and Gabor Angeli and Christopher Potts and Christopher D. Manning}, 
    title = {A large annotated corpus for learning natural language inference},
    year = {2015},
    note = {arXiv:1508.05326 [cs.CL]. Available at: \url{https://arxiv.org/abs/1508.05326}}
}

@inbook{dagan2006,
    author = {Dagan, I. and Glickman, O. and Magnini, B.},
    title = {The PASCAL Recognising Textual Entailment Challenge. In: Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment},
    publisher = {Springer, Berlin, Heidelberg},
    year = {2005},
    note = {\url{https://doi.org/10.1007/11736790_9}}
}

@article{PARAMASIVAM20229644,
title = {A survey on textual entailment based question answering},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {10, Part B},
pages = {9644-9653},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2021.11.017},
url = {https://www.sciencedirect.com/science/article/pii/S1319157821003311},
author = {Aarthi Paramasivam and S. Jaya Nirmala},
keywords = {Natural Language Processing, Question Answering, Textual Entailment},
abstract = {Question answering, an information retrieval system that seeks knowledge, is one of the classic applications in Natural Language Processing. A question answering system comprises numerous sets of subtasks. Some of the subtasks are Passage Retrieval, Answer Ranking, Question Similarity, Question Generation, Question Classification, Answer Selection, and Answer Validation. Numerous approaches have been experimented on in the question answering system to achieve accurate results. One such approach for the question answering system is Textual Entailment. Textual Entailment is a framework that captures significant semantic inference. Textual Entailment of two text fragments can be defined as the task of deciding whether the meaning of one text fragment can be inferred from another text fragment. This survey discusses how and why Textual Entailment is applied to various subtasks in question answering.}
}

@inproceedings{merrill-etal-2022-entailment,
    title = "Entailment Semantics Can Be Extracted from an Ideal Language Model",
    author = "Merrill, William  and
      Warstadt, Alex  and
      Linzen, Tal",
    editor = "Fokkens, Antske  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 26th Conference on Computational Natural Language Learning (CoNLL)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.conll-1.13/",
    doi = "10.18653/v1/2022.conll-1.13",
    pages = "176--193",
    abstract = "Language models are often trained on text alone, without additional grounding. There is debate as to how much of natural language semantics can be inferred from such a procedure. We prove that entailment judgments between sentences can be extracted from an ideal language model that has perfectly learned its target distribution, assuming the training sentences are generated by Gricean agents, i.e., agents who follow fundamental principles of communication from the linguistic theory of pragmatics. We also show entailment judgments can be decoded from the predictions of a language model trained on such Gricean data. Our results reveal a pathway for understanding the semantic information encoded in unlabeled linguistic data and a potential framework for extracting semantics from language models."
}

@inproceedings{bender-koller-2020-climbing,
    title = "Climbing towards {NLU}: {On} Meaning, Form, and Understanding in the Age of Data",
    author = "Bender, Emily M.  and
      Koller, Alexander",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.463/",
    doi = "10.18653/v1/2020.acl-main.463",
    pages = "5185--5198",
    abstract = "The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as {\textquotedblleft}understanding{\textquotedblright} language or capturing {\textquotedblleft}meaning{\textquotedblright}. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of {\textquotedblleft}Taking Stock of Where We{'}ve Been and Where We{'}re Going{\textquotedblright}, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding."
}

@article{hogarthFT,
    author = {Ian Hogarth},
    title = {We must slow down the race to god-like AI},
    journal = {Financial Times, 12/04/2023},
    year = {}
}

@book{brecht,
    author = {Bertolt Brecht},
    title = {Bertolt Brecht Werke: Schriften 3. Vol. 23.},
    publisher = {Berlin: Aufbau-Verlag},
    year = {1993},
    note = {Translation by Alex Bull}
}

@book{derrida1,
    author = {Jacques Derrida},
    title = {De la Grammatologie},
    publisher = {Les Éditions de Minuit},
    year = {1967}
}

@book{derrida2,
    author = {Jacques Derrida},
    title = {L'Écriture et la Différence},
    publisher = {Éditions du Seuil},
    year = {1967}
}

@book{derrida3,
    author = {Jacques Derrida},
    title = {Le monolinguisme de l'autre: ou la prothèse d'origine},
    publisher = {Éditions Galilée},
    year = {1996}
}